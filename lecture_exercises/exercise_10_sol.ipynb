{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "logfile = 'log.txt' ## name your log file.\n",
    "connector = scraping_class.Connector(logfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 10: Parsing and Information Extraction\n",
    "\n",
    "In this Exercise Set we shall develop our webscraping skills even further by practicing **parsing** and navigating html trees using BeautifoulSoup and furthermore train extracting information from raw text with no html tags to help, using regular expressions. \n",
    "\n",
    "But just as importantly you will get a chance to think about **data quality issues** and how to ensure reliability when curating your own webdata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 10.1: Logging and data quality\n",
    "\n",
    "> **Ex. 10.1.1:** *Why is is it important to log processes in your data collection?*\n",
    "\n",
    "** answer goes here** \n",
    "\n",
    "** solution** When designing your own data collection you are in charge of ensuring the quality. Many processes can go wrong, which if not spotted can lead to fundamental distortions of your dataset and in turn conclusions. \n",
    "\n",
    "> **Ex. 10.1.2:**\n",
    "*How does logging help with both ensuring and documenting the quality of your data?*\n",
    "** answer goes here** \n",
    "\n",
    "** solution** Having a log allow you to spot the potential anomalies in the website you are scraping. Examples could be errors and failed calls not randomly distributed accross time or subdomains. This would prompt you to investigate the issue further. Inspecting the simple distribution of lenght of the response, can also tell you about potential artifacts in your data collection that you should look into. Maybe there are more than one template important for your parsing, or there is a certain standard response given when data is missing.\n",
    "When presenting simple statistics about the data collection, it provides transparency and signals the commitment you have made as a data curator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 10.2: Parsing a Table from HTML using BeautifulSoup.\n",
    "\n",
    "Yesterday I showed you a neat little prepackaged function in pandas that did all the work. However today we should learn the mechanics of it. *(It is not just for educational purposes, sometimes the package will not do exactly as you want.)*\n",
    "\n",
    "We hit the Basketball stats page from yesterday again: https://www.basketball-reference.com/leagues/NBA_2018.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.1:** Here we practice simply locating the table node of interest using the `find` method build into BeautifoulSoup. But first we have to fetch the HTML using the `requests` module. Parse the tree using `BeautifulSoup`. And then use the **>Inspector<** tool (* right click on the table < press inspect element *) in your browser to see how to locate the Eastern Conference table node - i.e. the *tag* name of the node, and maybe some defining *attributes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://www.basketball-reference.com/leagues/NBA_2018.html'\n",
    "response = connector.get(url,'table_exercise')\n",
    "html = response[0].text\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "table_node = soup.find('table',attrs={'id':'confs_standings_E'}) # find table node with specific attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have located the table should now build a function that starts at a \"table node\" and parses the information, and outputs a pandas DataFrame. \n",
    "\n",
    "Inspect the element either within the notebook or through the **>Inspector<** tool and start to see how a table is written in html. Which tag names can be used to locate rows? How will you iterate through columns. Were is the header located?\n",
    "\n",
    "> **Ex. 10.1.2:** First you parse the header which can be found in the canonical tag name: thead. \n",
    "Next you use the `find_all` method to search for the tag, and iterate through each of the elements extracting the text, using the `.text` method builtin to the the node object. Store the header values in a list container. \n",
    "\n",
    "> **Ex. 10.1.3:** Next you locate the rows, using the canonical tag name: tbody. And from here you search for all rows tags. Fiugre out the tag name yourself, inspecting the tbody node in python or using the **Inspector**. \n",
    "\n",
    "> **Ex. 10.1.4:** Next run through all the rows and extract each value, similar to how you extracted the header. However here is a slight variation: Since each value node can have a different tag depending on whether it is a digit or a string, you should use the `.children` method instead of the `.find_all` - (or write compile a regex that matches both the td tag and the th tag.) \n",
    ">Once the value nodes of each row has been located using the `.children` method you should extract the value. Store the extracted rows as a list of lists: ```[[val1,val2,...valk],...]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eastern Conference', 'Wins', 'Losses', 'Win-Loss Percentage', 'Games Behind', 'Points Per Game', 'Opponent Points Per Game', 'Simple Rating System'] 8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Solution\n",
    "name = table_node.caption.text\n",
    "# parse header\n",
    "header = table_node.thead.find_all('th') # locate each column name using the `th` tag, which entail a string .\n",
    "# extract the label you want. brevity use .text, for a more informative get teh aria-label attribute\n",
    "header = [col['aria-label'] for col in header]\n",
    "print(header,len(header))\n",
    "# parse rows: the canonical tbody locates the data body.\n",
    "body = table_node.tbody\n",
    "# rows are found using the \"tr\" tag\n",
    "rows = body.find_all('tr')\n",
    "# each row value has different tags depending on their type (digit or string)\n",
    "# function to check what tag it is and either convert to float or not. \n",
    "import numpy as np\n",
    "def convert_value_type(value_node):\n",
    "    if value_node.name == 'th':\n",
    "        return value_node.text\n",
    "    else: # assume node is td:\n",
    "        try: \n",
    "            return float(value_node.text)\n",
    "        except:\n",
    "            return np.nan # assume there is no value if it fails. \n",
    "data = []\n",
    "for row_node in rows:\n",
    "    row = []\n",
    "    for child in row_node.children:\n",
    "        row.append(convert_value_type(child))\n",
    "    data.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eastern Conference</th>\n",
       "      <th>Wins</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Win-Loss Percentage</th>\n",
       "      <th>Games Behind</th>\n",
       "      <th>Points Per Game</th>\n",
       "      <th>Opponent Points Per Game</th>\n",
       "      <th>Simple Rating System</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toronto Raptors* (1)</td>\n",
       "      <td>59.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.7</td>\n",
       "      <td>103.9</td>\n",
       "      <td>7.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boston Celtics* (2)</td>\n",
       "      <td>55.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.671</td>\n",
       "      <td>4.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>100.4</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Philadelphia 76ers* (3)</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.634</td>\n",
       "      <td>7.0</td>\n",
       "      <td>109.8</td>\n",
       "      <td>105.3</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cleveland Cavaliers* (4)</td>\n",
       "      <td>50.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.610</td>\n",
       "      <td>9.0</td>\n",
       "      <td>110.9</td>\n",
       "      <td>109.9</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indiana Pacers* (5)</td>\n",
       "      <td>48.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>11.0</td>\n",
       "      <td>105.6</td>\n",
       "      <td>104.2</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Eastern Conference  Wins  Losses  Win-Loss Percentage  Games Behind  \\\n",
       "0      Toronto Raptors* (1)   59.0    23.0                0.720           NaN   \n",
       "1       Boston Celtics* (2)   55.0    27.0                0.671           4.0   \n",
       "2   Philadelphia 76ers* (3)   52.0    30.0                0.634           7.0   \n",
       "3  Cleveland Cavaliers* (4)   50.0    32.0                0.610           9.0   \n",
       "4       Indiana Pacers* (5)   48.0    34.0                0.585          11.0   \n",
       "\n",
       "   Points Per Game  Opponent Points Per Game  Simple Rating System  \n",
       "0            111.7                     103.9                  7.29  \n",
       "1            104.0                     100.4                  3.23  \n",
       "2            109.8                     105.3                  4.30  \n",
       "3            110.9                     109.9                  0.59  \n",
       "4            105.6                     104.2                  1.18  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(data,columns=header)\n",
    "def parse_html_table(table_node):\n",
    "    # parse header\n",
    "    header = table_node.thead.find_all('th') # locate each column name using the `th` tag, which entail a string .\n",
    "    # extract the label you want. brevity use .text, for a more informative get teh aria-label attribute\n",
    "    header = [col['aria-label'] for col in header]\n",
    "    # parse rows: the canonical tbody locates the data body.\n",
    "    body = table_node.tbody\n",
    "    # rows are found using the \"tr\" tag\n",
    "    rows = body.find_all('tr')\n",
    "    # each row value has different tags depending on their type (digit or string)\n",
    "    # function to check what tag it is and either convert to float or not. \n",
    "    import numpy as np\n",
    "    def convert_value_type(value_node):\n",
    "        if value_node.name == 'th':\n",
    "            return value_node.text\n",
    "        else: # assume node is td:\n",
    "            try: \n",
    "                return float(value_node.text)\n",
    "            except:\n",
    "                return np.nan # assume there is no value if it fails. \n",
    "    data = []\n",
    "    for row_node in rows:\n",
    "        row = []\n",
    "        for child in row_node.children:\n",
    "            row.append(convert_value_type(child))\n",
    "        data.append(row)\n",
    "    df = pd.DataFrame(data,columns=header)\n",
    "    return df\n",
    "df = parse_html_table(table_node)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.6:** Now locate all tables from the page, using the `.find_all` method searching for the table tag name. Iterate through the table nodes and apply the function created for parsing html tables. Store each table in a dictionary using the table name as key. The name is found by accessing the id attribute of each table node, using dictionary-style syntax - i.e. `table_node['id']`.\n",
    "\n",
    "> ** 10.2.extra.:** Compare your results to the pandas implementation. pd.read_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "tables = soup.find_all('table') # locate all table nodes\n",
    "dfs = {}\n",
    "for table_node in tables:\n",
    "    name = table_node['id'] # access the id attribute. \n",
    "    table = parse_html_table(table_node) # apply parse_table function\n",
    "    dfs[name] = table # store table in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 10.2: Practicing Regular Expressions.\n",
    "This exercise is about developing your experience with designing your own regular expressions.\n",
    "\n",
    "Remember you can always consult the regular expression reference page [here](https://www.regular-expressions.info/refquick.html), if you need to remember or understand a specific symbol. \n",
    "\n",
    "You should practice using *\"define-inspect-refine-method\"* described in the lectures to systematically ***explore*** and ***refine*** your expressions, and save all the patterns tried. You can download the small module that I created to handle this in the following way: \n",
    "``` python\n",
    "import requests\n",
    "url = 'https://raw.githubusercontent.com/snorreralund/explore_regex/master/explore_regex.py'\n",
    "response = requests.get(url)\n",
    "with open('explore_regex.py','w') as f:\n",
    "    f.write(response.text)\n",
    "import explore_regex as e_re\n",
    "```\n",
    "\n",
    "Remember to start ***broad*** to gain many examples, and iteratively narrow and refine.\n",
    "\n",
    "We will use a sample of the trustpilot dataset that you practiced collecting yesterday.\n",
    "You can load it directly into python from the following link: https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.0:** Load the data used in the exercise using the `pd.read_csv` function. (Hint: path to file can be both a url or systempath). \n",
    "\n",
    ">Define a variable `sample_string = '\\n'.join(df.sample(2000).reviewBody)` as sample of all the reviews that you will practice on.  (Run it once in a while to get a new sample for potential differences).\n",
    "Imagine we were a company wanting to find the reviews where customers are concerned with the price of a service. They decide to write a regular expression to match all reviews where a currencies and an amount is mentioned. \n",
    "\n",
    "> **Ex. 10.2.1:** \n",
    "> Write an expression that matches both the dollar-sign (\\$) and dollar written literally, and the amount before or after a dollar-sign. Remember that the \"$\"-sign is a special character in regular expressions. Explore and refine using the explore_pattern function in the package I created called explore_regex. \n",
    "```python\n",
    "import explore_regex as e_re\n",
    "explore_regex = e_re.Explore_Regex(sample_string) # Initaizlie the Explore regex Class.\n",
    "explore_regex.explore_pattern(pattern) # Use the .explore_pattern method.\n",
    "```\n",
    "\n",
    "\n",
    "Start with exploring the context around digits (\"\\d\") in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "# download data\n",
    "path2data = 'https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv'\n",
    "df = pd.read_csv(path2data)\n",
    "# download module\n",
    "url = 'https://raw.githubusercontent.com/snorreralund/explore_regex/master/explore_regex.py'\n",
    "response = requests.get(url)\n",
    "# write script to your folder to create a locate module\n",
    "with open('explore_regex.py','w') as f:\n",
    "    f.write(response.text)\n",
    "# import local module\n",
    "import explore_regex as e_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "digit_re = re.compile('[0-9]+') \n",
    "df['hasNumber'] = df.reviewBody.apply(lambda x: len(digit_re.findall(x))>0)\n",
    "sample_string = '\\n'.join(df[df['hasNumber']].sample(1000).reviewBody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_regex = e_re.ExploreRegex(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Pattern: [£$] ?[0-9]+(?:[.,][0-9]+)?|[0-9]+(?:[.,][0-9]+)? ?(?:USD|usd)|[0-9]+(?:[.,][0-9]+)? ?(?:dollars|DOLLARS)\t Matched 267 patterns -----\n",
      "Match: $100\tContext:t it’s $300 and the $100 of insurance doesn’\n",
      "Match: $20.00\tContext:yesterday, I put in $20.00 , but, when I tried\n",
      "Match: $100\tContext:shipping and 25 off $100 and that really sav\n",
      "Match: $10\tContext: other sites wanted $10. The only thing was\n",
      "Match: $25.00\tContext:k out. I bought the $25.00 gift cards from Fan\n",
      "Match: $200\tContext:m this cost me over $200.-. They told me aft\n",
      "Match: $9.98\tContext:pany at Walmart for $9.98. Could of saved tim\n",
      "Match: £78\tContext:ed the order on the £78 dress I'd wanted fo\n",
      "Match: $42\tContext:I do, you know that $42/night would not get\n",
      "Match: $18\tContext:the mattress needed $18 straps to hold it. \n"
     ]
    }
   ],
   "source": [
    "pattern = '[£$] ?[0-9]+(?:[.,][0-9]+)?|[0-9]+(?:[.,][0-9]+)? ?(?:USD|usd)|[0-9]+(?:[.,][0-9]+)? ?(?:dollars|DOLLARS)' #[£$] ?[0-9]+(?:[.,][0-9]+)?|[0-9]+(?:[.,][0-9]+)? ?(?:USD|usd)|\n",
    "#pattern = '[£$] ?[0-9]+(?:[.,][0-9]+)?|[0-9]+(?:[.,][0-9]+)? ?(?:USD|usd)|[0-9]+(?:[.,][0-9]+)? ?(dollars|DOLLARS)' # \n",
    "\n",
    "explore_regex.explore_pattern(pattern,context=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.10.2.3** Use the .report() method. e_re.report(), and print the all patterns in the development process using the .pattern method - i.e. e_re.patterns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Pattern: [£$] ?[0-9]+(?:[.,][0-9]+)?|[0-9]+(?:[.,][0-9]+)? ?(?:USD|usd)|[0-9]+(?:[.,][0-9]+)? ?(?:dollars|DOLLARS)\t Matched 267 patterns -----\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-91499c79cf89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#[Answer 10.2.3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexplore_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'soft'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/session_10 (2)/session_10/explore_regex.py\u001b[0m in \u001b[0;36mreport\u001b[0;34m(self, method, plot)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/session_10 (2)/session_10/explore_regex.py\u001b[0m in \u001b[0;36mplot_similarity\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n\u001b[1;32m    516\u001b[0m                           \u001b[0mannot_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                           yticklabels, mask)\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;31m# Add the pcolormesh kwargs here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Determine good default values for the colormapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         self._determine_cmap_params(plot_data, vmin, vmax,\n\u001b[0;32m--> 167\u001b[0;31m                                     cmap, center, robust)\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# Sort out the annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m_determine_cmap_params\u001b[0;34m(self, plot_data, vmin, vmax, cmap, center, robust)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mcalc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvmin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mvmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrobust\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcalc_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvmax\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m98\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrobust\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcalc_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amin\u001b[0;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[1;32m     30\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     31\u001b[0m           initial=_NoValue):\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#[Answer 10.2.3]\n",
    "explore_regex.report('soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[£$] ?[0-9]+(?:[.,][0-9]+)?|[0-9]+(?:[.,][0-9]+)? ?(?:USD|usd)|[0-9]+(?:[.,][0-9]+)? ?(?:dollars|DOLLARS)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_regex.patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">** Ex. 10.2.4** \n",
    "Finally write a function that takes in a string and outputs if there is a match. Use the .match function to see if there is a match (hint if does not return a NoneType object - `re.match(pattern,string)!=None`).\n",
    "\n",
    "> Define a column 'mention_currency' in the dataframe, by applying the above function to the text column of the dataframe. \n",
    "*** You should have approximately 310 reviews that matches. - but less is also alright***\n",
    "\n",
    "> **Ex. 10.2.5** Explore the relation between reviews mentioning prices and the average rating. \n",
    "\n",
    "> **Ex. 10.2.extra** Define a function that outputs the amount mentioned in the review (if more than one the largest), define a new column by applying it to the data, and explore whether reviews mentioning higher prices are worse than others by plotting the amount versus the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer to 10.2.4-5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '[£$] ?[0-9]+(?:[.,][0-9]+)?|[0-9]+(?:[.,][0-9]+)? ?(?:USD|usd)|[0-9]+(?:[.,][0-9]+)? ?(?:dollars|DOLLARS)'\n",
    "currency_re = re.compile(pattern)\n",
    "def match_currency(string):\n",
    "    return len(currency_re.findall(string))>0\n",
    "df['mention_currency'] = df.reviewBody.apply(match_currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mention_currency\n",
       "False    4.507275\n",
       "True     2.935275\n",
       "Name: reviewRating_ratingValue, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('mention_currency').reviewRating_ratingValue.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.5:** Now we write a regular expression to extract emoticons from text.\n",
    "Start by locating all mouths ')' of emoticons, and develop the variations from there. Remember that paranthesis are special characters in regex, so you should use the escape character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = '\\n'.join(df.sample(1000).reviewBody)\n",
    "emoticon_regex = e_re.ExploreRegex(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Pattern: [:;][-Oo]?[()D]\t Matched 5 patterns -----\n",
      "Match: :)\tContext:in really it worked :)\n",
      "Fast, hassle-free, \n",
      "Match: :)\tContext:y with my purchase! :)\n",
      "Townfair in Johnsto\n",
      "Match: :)\tContext:e with good bundles :)\n",
      "Planning a trip to \n",
      "Match: :)\tContext:always profesionals :)\n",
      "Nice room, nice loc\n",
      "Match: :)\tContext:que ideas possible. :)\n",
      "Outstanding service\n"
     ]
    }
   ],
   "source": [
    "pattern = '[:;][-Oo]?[()D]'\n",
    "emoticon_regex.explore_pattern(pattern,context=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emoticon_regex.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[:;][-Oo]?[()D]': 5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoticon_regex.pattern2n_match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
