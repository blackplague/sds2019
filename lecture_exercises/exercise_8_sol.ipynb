{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Connector class for accessing the internet\n",
    "Even if logging is not important for the below exercises, get in the habit of using this class for connecting to the internet, to practice logging your activity. This will be expected in the final exam.\n",
    "\n",
    "You should run `pip install scraping_class` to install the module to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "logfile = 'log.csv'## name your log file.\n",
    "connector = scraping_class.Connector(logfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 8: Introduction to Web Scraping\n",
    "\n",
    "*Afternoon, August 15, 2019*\n",
    "\n",
    "In this Exercise Set we shall practice our webscraping skills utiilizing only basic python. We shall cover variations between static and dynamic pages and build. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 8.1: Scraping Jobnet.dk\n",
    "\n",
    "This exercise you get to practice locating the request that the JavaScript sends to get the job data that it builds the joblistings from. You should use the **>Network Monitor<** tool in your browser.\n",
    "\n",
    "Furthermore you practice spotting how the pagination is done, without clicking on the next page button, but instead changing a small parameter in the URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.1:** Hit the joblisting webpage here: https://job.jobnet.dk/CV and locate the request that gets the joblisting data using the the **>Network Monitor<**. *(Hint: Filter by XHR files)  \n",
    "\n",
    "> **Ex. 8.1.2.:** Use the `request` module to collect the first 20 results and unpack the relevant `json` data into a `pandas` DataFrame.\n",
    "\n",
    "> **Ex. 8.1.3.:** Store the 'TotalResultCount' value for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 8.1.1-3 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "url = 'https://job.jobnet.dk/CV/FindWork/Search?SortValue=CreationDate&Offset=0'\n",
    "response,call_id = connector.get(url,'mapping_jobposting')\n",
    "if response.ok:\n",
    "    d = response.json()\n",
    "else:\n",
    "    print('error')\n",
    "df = pd.DataFrame(d['JobPositionPostings'])\n",
    "df.head()\n",
    "n_listings = d['TotalResultCount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.1.4:** This exercise is about paging the results. We need to understand the websites pagination scheme. \n",
    "\n",
    "> Now scroll down the webpage and press the next page button. See how the parameters of the url changes as you turn the pages.\n",
    "\n",
    "> **Ex. 8.1.5:** Design a`for` loop using the `range` function that changes this paging parameter in the URL. Use the TotalResultCount parameter from before to define the limits of the range function. Store these urls in a container. \n",
    "\n",
    ">**extra** Change the SortValue parameter from BestMatch to CreationDate, to make the sorting amendable to updating results daily.\n",
    "\n",
    "*(HINT: See that the parameter is an offset and that this relates to the number of results pr. call made.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 8.1.4-5 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'https://job.jobnet.dk/CV/FindWork/Search?SortValue=CreationDate&Offset=%d'\n",
    "links = []\n",
    "for offset in range(0,n_listings+20,20):\n",
    "    url = q%offset\n",
    "    links.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.8.1.6:** Pick 20 random links using the `random.sample()` function and collect them using the `Connector` class. Also use the `time.sleep()` function to limit the rate of your calls. Make sure to save the links already collected in a `set()` container to avoid having to reload links already collected. ***extra***: monitor the time left to completing the loop by using `tqdm.tqdm()` function.\n",
    "\n",
    "> **Ex.8.1.7:** Load all the results into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 8.1.6-7 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:25<00:00,  1.27s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AnonymousEmployer</th>\n",
       "      <th>AssignmentStartDate</th>\n",
       "      <th>AutomatchType</th>\n",
       "      <th>Country</th>\n",
       "      <th>DetailsUrl</th>\n",
       "      <th>EmploymentType</th>\n",
       "      <th>HasLocationValues</th>\n",
       "      <th>HiringOrgCVR</th>\n",
       "      <th>HiringOrgName</th>\n",
       "      <th>ID</th>\n",
       "      <th>...</th>\n",
       "      <th>UseWorkPlaceAddressForJoblog</th>\n",
       "      <th>Weight</th>\n",
       "      <th>WorkHours</th>\n",
       "      <th>WorkPlaceAbroad</th>\n",
       "      <th>WorkPlaceAddress</th>\n",
       "      <th>WorkPlaceCity</th>\n",
       "      <th>WorkPlaceNotStatic</th>\n",
       "      <th>WorkPlaceOtherAddress</th>\n",
       "      <th>WorkPlacePostalCode</th>\n",
       "      <th>WorkplaceID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>False</td>\n",
       "      <td>0001-01-01T00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Danmark</td>\n",
       "      <td>https://job.jobnet.dk/CV/FindWork/Details/5028604</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>40887113</td>\n",
       "      <td>Ingeborggården</td>\n",
       "      <td>5028604</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fuldtid</td>\n",
       "      <td>False</td>\n",
       "      <td>Troels-Lunds Vej 27-29</td>\n",
       "      <td>Frederiksberg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2000</td>\n",
       "      <td>36565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>False</td>\n",
       "      <td>0001-01-01T00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Danmark</td>\n",
       "      <td>https://job.jobnet.dk/CV/FindWork/Details/5032068</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>35391282</td>\n",
       "      <td>TRADESHIFT ApS</td>\n",
       "      <td>5032068</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fuldtid</td>\n",
       "      <td>False</td>\n",
       "      <td>Landemærket 10 1</td>\n",
       "      <td>København K</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>False</td>\n",
       "      <td>2019-09-19T10:36:15.3511693+02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Danmark</td>\n",
       "      <td>None</td>\n",
       "      <td>Fastansættelse</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Comwell Køge Strand</td>\n",
       "      <td>E7190234</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fuldtid</td>\n",
       "      <td>False</td>\n",
       "      <td>Strandvejen 111</td>\n",
       "      <td>Køge</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>False</td>\n",
       "      <td>0001-01-01T00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Danmark</td>\n",
       "      <td>https://job.jobnet.dk/CV/FindWork/Details/5022640</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>29979812</td>\n",
       "      <td>KU-SUND-Metabolismecentret</td>\n",
       "      <td>5022640</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fuldtid</td>\n",
       "      <td>False</td>\n",
       "      <td>Nørre Allé 20</td>\n",
       "      <td>København N</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>False</td>\n",
       "      <td>0001-01-01T00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Danmark</td>\n",
       "      <td>https://job.jobnet.dk/CV/FindWork/Details/5028453</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>37812048</td>\n",
       "      <td>LivaRosa ApS</td>\n",
       "      <td>5028453</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Deltid</td>\n",
       "      <td>False</td>\n",
       "      <td>Frederiks Allé 34</td>\n",
       "      <td>Aarhus C</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8000</td>\n",
       "      <td>128652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AnonymousEmployer                AssignmentStartDate  AutomatchType  \\\n",
       "337              False                0001-01-01T00:00:00              0   \n",
       "55               False                0001-01-01T00:00:00              0   \n",
       "397              False  2019-09-19T10:36:15.3511693+02:00              0   \n",
       "135              False                0001-01-01T00:00:00              0   \n",
       "326              False                0001-01-01T00:00:00              0   \n",
       "\n",
       "     Country                                         DetailsUrl  \\\n",
       "337  Danmark  https://job.jobnet.dk/CV/FindWork/Details/5028604   \n",
       "55   Danmark  https://job.jobnet.dk/CV/FindWork/Details/5032068   \n",
       "397  Danmark                                               None   \n",
       "135  Danmark  https://job.jobnet.dk/CV/FindWork/Details/5022640   \n",
       "326  Danmark  https://job.jobnet.dk/CV/FindWork/Details/5028453   \n",
       "\n",
       "     EmploymentType  HasLocationValues  HiringOrgCVR  \\\n",
       "337                               True      40887113   \n",
       "55                                True      35391282   \n",
       "397  Fastansættelse               True             0   \n",
       "135                               True      29979812   \n",
       "326                               True      37812048   \n",
       "\n",
       "                  HiringOrgName        ID  ...  UseWorkPlaceAddressForJoblog  \\\n",
       "337              Ingeborggården   5028604  ...                          True   \n",
       "55               TRADESHIFT ApS   5032068  ...                          True   \n",
       "397         Comwell Køge Strand  E7190234  ...                          True   \n",
       "135  KU-SUND-Metabolismecentret   5022640  ...                          True   \n",
       "326                LivaRosa ApS   5028453  ...                          True   \n",
       "\n",
       "     Weight  WorkHours WorkPlaceAbroad        WorkPlaceAddress  WorkPlaceCity  \\\n",
       "337     1.0    Fuldtid           False  Troels-Lunds Vej 27-29  Frederiksberg   \n",
       "55      1.0    Fuldtid           False        Landemærket 10 1    København K   \n",
       "397     1.0    Fuldtid           False         Strandvejen 111           Køge   \n",
       "135     1.0    Fuldtid           False           Nørre Allé 20    København N   \n",
       "326     1.0     Deltid           False       Frederiks Allé 34       Aarhus C   \n",
       "\n",
       "    WorkPlaceNotStatic WorkPlaceOtherAddress  WorkPlacePostalCode WorkplaceID  \n",
       "337              False                 False                 2000       36565  \n",
       "55               False                 False                 1119           0  \n",
       "397              False                 False                 4600           0  \n",
       "135              False                 False                 2200           0  \n",
       "326              False                 False                 8000      128652  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "done = set()\n",
    "data = []\n",
    "import tqdm\n",
    "for url in tqdm.tqdm(random.sample(links,20)):\n",
    "    response,call_id = connector.get(url,'download_jobposting')\n",
    "    if response.ok:\n",
    "        d = response.json()\n",
    "    else:\n",
    "        print('error')\n",
    "    data += d['JobPositionPostings']\n",
    "    time.sleep(0.5)\n",
    "df = pd.DataFrame(data)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 8.2: Scraping Trustpilot.com\n",
    "Now for a slightly more elaborate, yet still simple scraping problem. Here we want to scrape trustpilot for user reviews. This data is very nice since it provides free labeled data (rating) to train a machine learning model to understand positive and negative sentiment. \n",
    "\n",
    "Here you will practice crawling a website collecting the links to each company review page, and finally locate another behind the scenes JavaScript request that gets the review data in a neat json format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.1:** Visit the https://www.trustpilot.com/ website and locate the categories page.\n",
    "From this page you find links to company listings.\n",
    "\n",
    "> **Ex. 8.2.2:**\n",
    "Get the category page using the `requests` module and extract each link to a specific category page from the HTML. This can be done using the basic python `.split()` string method. Make sure only links within the ***/categories/*** section are kept, checking each string using the ```if 'pattern' in string``` condition. \n",
    "\n",
    "*(Hint: The links are relative. You need to add the domain name)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 8.2.1-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1316 /categories/bed_shop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.trustpilot.com/categories/bed_shop',\n",
       " 'https://www.trustpilot.com/categories/pet_trainer',\n",
       " 'https://www.trustpilot.com/categories/discount_supermarket',\n",
       " 'https://www.trustpilot.com/categories/media_company',\n",
       " 'https://www.trustpilot.com/categories/computer_consultant',\n",
       " 'https://www.trustpilot.com/categories/hair_extensions_supplier',\n",
       " 'https://www.trustpilot.com/categories/magic_store',\n",
       " 'https://www.trustpilot.com/categories/ophthalmologist',\n",
       " 'https://www.trustpilot.com/categories/book_store',\n",
       " 'https://www.trustpilot.com/categories/occupational_health_service']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.trustpilot.com/categories/'\n",
    "response,call_id = connector.get(url,'mapping_categories')\n",
    "if response.ok:\n",
    "    html = response.text\n",
    "else:\n",
    "    print('error')\n",
    "links = set()\n",
    "for link_loc in html.split('href=\"')[1:]:\n",
    "    link = link_loc.split('\"')[0]\n",
    "    if '/categories/' in link:\n",
    "        links.add(link)\n",
    "print(len(links),list(links)[0]) # link is relative\n",
    "links = ['https://www.trustpilot.com'+link for link in links]# add the domain to each link\n",
    "links[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.3:** Get one of the category section links. Write a function to extract the links to the company review page from the HTML.\n",
    "\n",
    "> **Ex. 8.2.4:** Figure out how the pagination is done, by following how the url changes when pressing the **next page**-button to obtain more company listings. Write a function that builds links to paging all the company listing results of each category. This includes parsing the number of subpages of each category and changing the correct parameter in the url.\n",
    "\n",
    "(Hint: Find the maximum number of result pages, right before the next page button and make a loop change the page parameter of the url.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer to Ex.8.2.3-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,os,time\n",
    "def ratelimit(dt):\n",
    "    \"A function that handles the rate of your calls.\"\n",
    "    time.sleep(dt) # sleep one second.\n",
    "\n",
    "class Connector():\n",
    "  def __init__(self,logfile,overwrite_log=False,connector_type='requests',session=False,path2selenium='',n_tries = 5,timeout=30,waiting_time=0.5):\n",
    "    \"\"\"This Class implements a method for reliable connection to the internet and monitoring. \n",
    "    It handles simple errors due to connection problems, and logs a range of information for basic quality assessments\n",
    "    \n",
    "    Keyword arguments:\n",
    "    logfile -- path to the logfile\n",
    "    overwrite_log -- bool, defining if logfile should be cleared (rarely the case). \n",
    "    connector_type -- use the 'requests' module or the 'selenium'. Will have different since the selenium webdriver does not have a similar response object when using the get method, and monitoring the behavior cannot be automated in the same way.\n",
    "    session -- requests.session object. For defining custom headers and proxies.\n",
    "    path2selenium -- str, sets the path to the geckodriver needed when using selenium.\n",
    "    n_tries -- int, defines the number of retries the *get* method will try to avoid random connection errors.\n",
    "    timeout -- int, seconds the get request will wait for the server to respond, again to avoid connection errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Initialization function defining parameters. \n",
    "    self.n_tries = n_tries # For avoiding triviel error e.g. connection errors, this defines how many times it will retry.\n",
    "    self.timeout = timeout # Defining the maximum time to wait for a server to response.\n",
    "    self.waiting_time = waiting_time # define simple rate_limit parameter.\n",
    "    ## not implemented here, if you use selenium.\n",
    "    if connector_type=='selenium':\n",
    "      assert path2selenium!='', \"You need to specify the path to you geckodriver if you want to use Selenium\"\n",
    "      from selenium import webdriver \n",
    "      ## HIN download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases\n",
    "\n",
    "      assert os.path.isfile(path2selenium),'You need to insert a valid path2selenium the path to your geckodriver. You can download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases'\n",
    "      self.browser = webdriver.Firefox(executable_path=path2selenium) # start the browser with a path to the geckodriver.\n",
    "\n",
    "    self.connector_type = connector_type # set the connector_type\n",
    "    \n",
    "    if session: # set the custom session\n",
    "      self.session = session\n",
    "    else:\n",
    "      self.session = requests.session()\n",
    "    self.logfilename = logfile # set the logfile path\n",
    "    ## define header for the logfile\n",
    "    header = ['id','project','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "    if os.path.isfile(logfile):        \n",
    "      if overwrite_log==True:\n",
    "        self.log = open(logfile,'w')\n",
    "        self.log.write(';'.join(header))\n",
    "      else:\n",
    "        self.log = open(logfile,'a')\n",
    "    else:\n",
    "      self.log = open(logfile,'w')\n",
    "      self.log.write(';'.join(header))\n",
    "    ## load log \n",
    "    with open(logfile,'r') as f: # open file\n",
    "        \n",
    "      l = f.read().split('\\n') # read and split file by newlines.\n",
    "      ## set id\n",
    "      if len(l)<=1:\n",
    "        self.id = 0\n",
    "      else:\n",
    "        self.id = int(l[-1][0])+1\n",
    "            \n",
    "  def get(self,url,project_name):\n",
    "    \"\"\"Method for connector reliably to the internet, with multiple tries and simple error handling, as well as default logging function.\n",
    "    Input url and the project name for the log (i.e. is it part of mapping the domain, or is it the part of the final stage in the data collection).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    url -- str, url\n",
    "    project_name -- str, Name used for analyzing the log. Use case could be the 'Mapping of domain','Meta_data_collection','main data collection'. \n",
    "    \"\"\"\n",
    "     \n",
    "    project_name = project_name.replace(';','-') # make sure the default csv seperator is not in the project_name.\n",
    "    if self.connector_type=='requests': # Determine connector method.\n",
    "      for _ in range(self.n_tries): # for loop defining number of retries with the requests method.\n",
    "        ratelimit(self.waiting_time)\n",
    "        t = time.time()\n",
    "        try: # error handling \n",
    "          response = self.session.get(url,timeout = self.timeout) # make get call\n",
    "\n",
    "          err = '' # define python error variable as empty assumming success.\n",
    "          success = True # define success variable\n",
    "          redirect_url = response.url # log current url, after potential redirects \n",
    "          dt = t - time.time() # define delta-time waiting for the server and downloading content.\n",
    "          size = len(response.text) # define variable for size of html content of the response.\n",
    "          response_code = response.status_code # log status code.\n",
    "          ## log...\n",
    "          call_id = self.id # get current unique identifier for the call\n",
    "          self.id+=1 # increment call id\n",
    "          #['id','project_name','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row to be written in the log.\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write log.\n",
    "          self.log.flush()\n",
    "          return response,call_id # return response and unique identifier.\n",
    "\n",
    "        except Exception as e: # define error condition\n",
    "          err = str(e) # python error\n",
    "          response_code = '' # blank response code \n",
    "          success = False # call success = False\n",
    "          size = 0 # content is empty.\n",
    "          redirect_url = '' # redirect url empty \n",
    "          dt = t - time.time() # define delta t\n",
    "\n",
    "          ## log...\n",
    "          call_id = self.id # define unique identifier\n",
    "          self.id+=1 # increment call_id\n",
    "\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write row to log.\n",
    "          self.log.flush()\n",
    "    else:\n",
    "      t = time.time()\n",
    "      ratelimit(self.waiting_time)\n",
    "      self.browser.get(url) # use selenium get method\n",
    "      ## log\n",
    "      call_id = self.id # define unique identifier for the call. \n",
    "      self.id+=1 # increment the call_id\n",
    "      err = '' # blank error message\n",
    "      success = '' # success blank\n",
    "      redirect_url = self.browser.current_url # redirect url.\n",
    "      dt = t - time.time() # get time for get method ... NOTE: not necessarily the complete load time.\n",
    "      size = len(self.browser.page_source) # get size of content ... NOTE: not necessarily correct, since selenium works in the background, and could still be loading.\n",
    "      response_code = '' # empty response code.\n",
    "      row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row \n",
    "      self.log.write('\\n'+';'.join(map(str,row))) # write row to log file.\n",
    "      self.log.flush()\n",
    "    # Using selenium it will not return a response object, instead you should call the browser object of the connector.\n",
    "    ## connector.browser.page_source will give you the html.\n",
    "      return None,call_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = Connector(logfile='trustpilot.csv', connector_type='selenium', path2selenium='./geckodriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: Failed to decode response from marionette\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-37baf39ab7bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.trustpilot.com/categories/book_store'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trsut-test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-e9d5a9d6d51c>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, project_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mratelimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiting_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use selenium get method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m       \u001b[0;31m## log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0mcall_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;31m# define unique identifier for the call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \"\"\"\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: Failed to decode response from marionette\n"
     ]
    }
   ],
   "source": [
    "conn.get('https://www.trustpilot.com/categories/book_store', 'trsut-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"totalNumberOfFilteredBusinesses\":0,\"totalNumberOfRecentlyReviewedBusinesses\":2,\"totalPageNumber\":0,\"filteredBusinessUnitList\":[],\"recentlyReviewedBusinessUnitList\":[{\"numberOfReviews\":2.0,\"identifyingName\":\"cheapfood.co.uk\",\"trustscore\":\"6.1\",\"displayName\":\"Cheapfood\",\"businessUnitId\":\"5bf03ed4b1ab5e000108c7b9\",\"stars\":3.0},{\"numberOfReviews\":0.0,\"identifyingName\":\"reducedtoclear.co.nz\",\"trustscore\":\"0.0\",\"displayName\":\"Reduced to Clear\",\"businessUnitId\":\"5d5615f1b38e4c000162c671\",\"stars\":1.0}]}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.trustpilot.com/categories/discount_supermarket/businesses?numberofreviews=0&page=1&status=collecting&timeperiod=12'\n",
    "response, _ = connector.get(url,'mapping')\n",
    "if response.ok:\n",
    "    html = response.text\n",
    "else:\n",
    "    print('error')\n",
    "\n",
    "def get_links(html):\n",
    "    links = set() # define container\n",
    "    for link_loc in html.split('href=\"')[1:]: # locate the start of a link\n",
    "        link = link_loc.split('\"')[0] # split at the end of the link\n",
    "        links.add(link) # if it is: add it to the set container\n",
    "    return links\n",
    "def get_company_links(html):\n",
    "    print(html)\n",
    "    link_collection = get_links(html)\n",
    "    company_links = [link for link in get_links(html) if '/review/' in link_collection] # check if the /review/ pattern is in the link\n",
    "    return company_links\n",
    "company_links = get_company_links(html)\n",
    "print(len(company_links))\n",
    "def get_all_category_pages(category_link):\n",
    "    response, _ = connector.get(category_link,'mapping_categories')\n",
    "    if response.ok:\n",
    "        html = response.text\n",
    "    else:\n",
    "        print('error')\n",
    "        return False\n",
    "    links = get_links(html)\n",
    "    # find the max_page.\n",
    "    page_links = [link for link in links if '?page=' in link] # check if the paging parameter is in the link\n",
    "    if len(page_links)==0: # no pages.\n",
    "        return [category_link]\n",
    "    n_pages = max([int(link.split('page=')[-1]) for link in page_links]) # extract the page value and take the max\n",
    "    paging_links = [category_link] # define container and store the original result page\n",
    "    q = category_link+'?page=%d' # define the varying parameter string.\n",
    "    for num in range(2,n_pages+1): # build the links.\n",
    "        paging_links.append(q%num)\n",
    "    return paging_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 8.2.5:** Loop through all categories and build the paging links using the above defined function.\n",
    "\n",
    "> **Ex. 8.2.6:** Randomly pick one of category listing links you have generated, and get the links to the companies listed using the other function defined. \n",
    "\n",
    "> **Ex. 8.2.7:** Visit one of these links and inspect the **>Network Monitor<** to locate the request that loads the review data. Use the requests module to retrieve this link and unpack the json results to a pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[Answer to Ex.8.2.5-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex. 8.2.5. Build the paging links\n",
    "company_listings = []\n",
    "for link in links: \n",
    "    if 'support.trustpilot' in link:\n",
    "        continue\n",
    "    company_listings+=get_all_category_pages(link)\n",
    "print('We need to visit %d company listing pages to collect all company addresses'%len(company_listings))\n",
    "# ex. 8.2.6\n",
    "company_links = get_company_links(random.choice(company_listings)) # use the above defined function\n",
    "# ex. 8.2.6\n",
    "direct_link = 'https://www.trustpilot.com/review/59f33de00000ff0005aed171/jsonld'\n",
    "response, call_id = connector.get(direct_link, 'download_review')\n",
    "d = response.json() # parse json using the build-in function.\n",
    "df = pd.DataFrame(d[0]['review'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on coming this far. By now you are almost - still need to figure out how to page the reviews and to find the company ID in the html -, ready to deploy a scraper collecting all reviews on trustpilot. \n",
    "If you wanna see just how valuable such data could be visit the follow blogpost: https://blog.openai.com/unsupervised-sentiment-neuron/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
